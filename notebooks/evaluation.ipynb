{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfac6d64-16e0-48f2-8a5e-4ac9bb42fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import trange\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings_input = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)  # linear layer\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        embeds = self.embeddings_input(input_word)\n",
    "        out = self.linear(embeds)  # nonlinear + projection\n",
    "        log_probs = F.log_softmax(out, dim=1)  # softmax compute log probability\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings_input = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)  # linear layer\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        embeds = self.embeddings_input(input_word)\n",
    "        embeds = torch.sum(embeds, dim=1)\n",
    "        out = self.linear(embeds)  # nonlinear + projection\n",
    "        log_probs = F.log_softmax(out, dim=1)  # softmax compute log probability\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, x_max = 100):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.x_max = x_max\n",
    "        self._focal_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self._context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self._focal_biases = nn.Embedding(vocab_size, 1).type(torch.float64)\n",
    "        self._context_biases = nn.Embedding(vocab_size, 1).type(torch.float64)\n",
    "\n",
    "    def forward(self, focal_input, context_input, coocurrence_count):\n",
    "        x_max = max(self.x_max, 1)\n",
    "        focal_embed = self._focal_embeddings(focal_input) # embed layer\n",
    "        context_embed = self._context_embeddings(context_input) # embed layer\n",
    "        focal_bias = self._focal_biases(focal_input) # bias for each embedding\n",
    "        context_bias = self._context_biases(context_input) # bias for each embedding\n",
    "\n",
    "        # count weight factor\n",
    "        weight_factor = torch.pow(coocurrence_count / x_max, 0.75)\n",
    "        weight_factor[weight_factor > 1] = 1\n",
    "\n",
    "        embedding_products = torch.sum(focal_embed * context_embed, dim=1)\n",
    "        log_cooccurrences = torch.log(coocurrence_count)\n",
    "\n",
    "        distance_expr = (\n",
    "            embedding_products + focal_bias + context_bias + log_cooccurrences\n",
    "        ) ** 2\n",
    "\n",
    "        single_losses = weight_factor * distance_expr\n",
    "        mean_loss = torch.mean(single_losses)\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329371cc-3f92-44c8-9f2d-cd1b96ac4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = \"../data/SimLex-999\"\n",
    "test_dataset = pd.read_csv(f\"{TEST_DIR}/SimLex-999.txt\", sep =\"\\t\")\n",
    "scaler = StandardScaler()\n",
    "test_dataset[['SimLex999']] = scaler.fit_transform(test_dataset[['SimLex999']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eeb88ea-e4cd-4862-b50d-62653ec58a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(model, name, idx):\n",
    "    if name == \"glove\":\n",
    "        emb = model._focal_embeddings(torch.tensor(idx))\n",
    "    else:\n",
    "        emb = model.embeddings_input(torch.tensor(idx))\n",
    "    return emb.cpu().detach().numpy()\n",
    "\n",
    "def evaluate(model_name: str, neg: str):\n",
    "    MODEL_DIR = \"../model\"\n",
    "    saved_params = torch.load(f\"{MODEL_DIR}/model_{model_name}_neg{neg}.pth\", map_location=torch.device('cpu'))\n",
    "    word_idx = saved_params[\"word_to_ix\"]\n",
    "    \n",
    "    if model_name == \"cbow\":\n",
    "        model = CBOWModel(100, len(word_idx))\n",
    "    elif model_name == \"skipgram\":\n",
    "        model = SkipGramModel(100, len(word_idx))\n",
    "    else:\n",
    "        model = GloVeModel(100, len(word_idx))\n",
    "        \n",
    "    model.load_state_dict(saved_params[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    true = []\n",
    "    def check(p, arr):\n",
    "        if p > 0:\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(-1)\n",
    "\n",
    "    for i in range(test_dataset.shape[0]):\n",
    "        w1 = test_dataset.iloc[i].word1\n",
    "        w2 = test_dataset.iloc[i].word2\n",
    "        if word_idx.get(w1, 0) and word_idx.get(w2, 0):\n",
    "            spr = spearmanr(get_emb(model, model_name, word_idx[w1]), get_emb(model,model_name, word_idx[w2]))\n",
    "            sim = test_dataset.iloc[i].SimLex999\n",
    "            check(spr.correlation, pred)\n",
    "            check(sim, true)\n",
    "\n",
    "    wrong = 0\n",
    "    for p,q in zip(pred, true):\n",
    "        if p != q:\n",
    "            wrong += 1\n",
    "    print(f\"{model_name} with {neg} negative samples => {wrong/len(pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c82ae3-bbff-40c9-9b91-9ecf3d5f316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow with 0 negative samples => 0.516597510373444\n",
      "cbow with 10 negative samples => 0.5031120331950207\n",
      "skipgram with 0 negative samples => 0.5010373443983402\n",
      "skipgram with 10 negative samples => 0.4823651452282158\n",
      "glove with 0 negative samples => 0.487551867219917\n",
      "glove with 10 negative samples => 0.5176348547717843\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"cbow\", 0)\n",
    "evaluate(\"cbow\", 10)\n",
    "\n",
    "evaluate(\"skipgram\", 0)\n",
    "evaluate(\"skipgram\", 10)\n",
    "\n",
    "evaluate(\"glove\", 0)\n",
    "evaluate(\"glove\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabba164-b980-45f2-b5a4-6baecf4a9333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
