{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec21f2c-3261-4858-803f-74c51b6c12b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Model and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b159722c-03bd-4b10-bda3-1e80528e17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import trange\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings_input = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)  # linear layer\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        embeds = self.embeddings_input(input_word)\n",
    "        out = self.linear(embeds)  # nonlinear + projection\n",
    "        log_probs = F.log_softmax(out, dim=1)  # softmax compute log probability\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings_input = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)  # linear layer\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        embeds = self.embeddings_input(input_word)\n",
    "        embeds = torch.sum(embeds, dim=1)\n",
    "        out = self.linear(embeds)  # nonlinear + projection\n",
    "        log_probs = F.log_softmax(out, dim=1)  # softmax compute log probability\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, x_max = 100):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.x_max = x_max\n",
    "        self._focal_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self._context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self._focal_biases = nn.Embedding(vocab_size, 1).type(torch.float64)\n",
    "        self._context_biases = nn.Embedding(vocab_size, 1).type(torch.float64)\n",
    "\n",
    "    def forward(self, focal_input, context_input, coocurrence_count):\n",
    "        x_max = max(self.x_max, 1)\n",
    "        focal_embed = self._focal_embeddings(focal_input) # embed layer\n",
    "        context_embed = self._context_embeddings(context_input) # embed layer\n",
    "        focal_bias = self._focal_biases(focal_input) # bias for each embedding\n",
    "        context_bias = self._context_biases(context_input) # bias for each embedding\n",
    "\n",
    "        # count weight factor\n",
    "        weight_factor = torch.pow(coocurrence_count / x_max, 0.75)\n",
    "        weight_factor[weight_factor > 1] = 1\n",
    "\n",
    "        embedding_products = torch.sum(focal_embed * context_embed, dim=1)\n",
    "        log_cooccurrences = torch.log(coocurrence_count)\n",
    "\n",
    "        distance_expr = (\n",
    "            embedding_products + focal_bias + context_bias + log_cooccurrences\n",
    "        ) ** 2\n",
    "\n",
    "        single_losses = weight_factor * distance_expr\n",
    "        mean_loss = torch.mean(single_losses)\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ba0ab6-2fec-41a1-aae8-d4c7430c4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(model, name, idx):\n",
    "    if name == \"glove\":\n",
    "        emb = model._focal_embeddings(torch.tensor(idx))\n",
    "    else:\n",
    "        emb = model.embeddings_input(torch.tensor(idx))\n",
    "    return emb.cpu().detach().numpy()\n",
    "\n",
    "def save_word_embeddings(model_name: str, neg: str):\n",
    "    MODEL_DIR = \"../model\"\n",
    "    saved_params = torch.load(f\"{MODEL_DIR}/model_{model_name}_neg{neg}.pth\", map_location=torch.device('cpu'))\n",
    "    word_idx = saved_params[\"word_to_ix\"]\n",
    "    \n",
    "    if model_name == \"cbow\":\n",
    "        model = CBOWModel(100, len(word_idx))\n",
    "    elif model_name == \"skipgram\":\n",
    "        model = SkipGramModel(100, len(word_idx))\n",
    "    else:\n",
    "        model = GloVeModel(100, len(word_idx))\n",
    "        \n",
    "    model.load_state_dict(saved_params[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    word_embeddings = defaultdict(list)\n",
    "    for w in word_idx.keys():\n",
    "        word_embeddings[w] = get_emb(model, model_name, neg)\n",
    "    with open(f\"{MODEL_DIR}/{model_name}_{neg}.embedding\", \"wb\") as f:\n",
    "        pickle.dump(word_embeddings, f, pickle.DEFAULT_PROTOCOL)\n",
    "    print(f\"{model_name} embeddings saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070fe531-9f39-4192-87f4-c8ac9764352d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864b8fcc-309e-42d5-aad3-3efa5d78732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow embeddings saved\n",
      "cbow embeddings saved\n",
      "skipgram embeddings saved\n",
      "skipgram embeddings saved\n",
      "glove embeddings saved\n",
      "glove embeddings saved\n"
     ]
    }
   ],
   "source": [
    "for m in [\"cbow\", \"skipgram\", \"glove\"]:\n",
    "    for n in [0, 10]:\n",
    "        save_word_embeddings(m, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
